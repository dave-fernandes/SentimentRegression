{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing for Market Predictions\n",
    "\n",
    "> Copyright 2019 Dave Fernandes. All Rights Reserved.\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "> you may not use this file except in compliance with the License.\n",
    "> You may obtain a copy of the License at\n",
    ">\n",
    "> http://www.apache.org/licenses/LICENSE-2.0\n",
    ">  \n",
    "> Unless required by applicable law or agreed to in writing, software\n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "> See the License for the specific language governing permissions and\n",
    "> limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data files can be downloaded from: https://www.kaggle.com/aaron7sun/stocknews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load news content and clean up strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_CSV = './data/RedditNews.csv'\n",
    "\n",
    "def clean_string(s):\n",
    "    s = ' '.join(s.splitlines())\n",
    "    \n",
    "    if s[0] == 'b':\n",
    "        cleaned_s = s[2:-1]\n",
    "    else:\n",
    "        cleaned_s = s\n",
    "    \n",
    "    return cleaned_s\n",
    "\n",
    "# Create dictionary of lists of news articles keyed by date string\n",
    "news_by_date = {}\n",
    "\n",
    "with open(NEWS_CSV) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    \n",
    "    for row in reader:\n",
    "        date = row[0]\n",
    "        string = row[1]\n",
    "        \n",
    "        if date in news_by_date:\n",
    "            string_list = news_by_date[date]\n",
    "        elif date != 'Date':\n",
    "            string_list = []\n",
    "            news_by_date[date] = string_list\n",
    "        else:\n",
    "            string_list = None\n",
    "        \n",
    "        if string_list != None:\n",
    "            string_list.append(clean_string(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load stock index values and compute derived quantities to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCKS_CSV = './data/DJIA_table.csv'\n",
    "\n",
    "# Column indices from original data\n",
    "DATE = 0\n",
    "OPEN = 1\n",
    "HIGH = 2\n",
    "LOW = 3\n",
    "CLOSE = 4\n",
    "VOLUME = 5\n",
    "ADJ_CLOSE = 6\n",
    "\n",
    "# Column indices for derived data\n",
    "DAYS_SINCE_LAST_TRADE = 0  # Days elapsed since previous trading day\n",
    "DAYS_UNTIL_NEXT_TRADE = 1  # Days until next trading day\n",
    "OPEN_LOG_RATIO        = 2  # Log of ratio of adjusted open to previous day's adjusted close\n",
    "CLOSE_LOG_RATIO       = 3  # Log of ratio of adjusted close to previous day's adjusted close\n",
    "HIGH_LOG_RATIO        = 4  # Log of ratio of adjusted high to previous day's adjusted close\n",
    "LOW_LOG_RATIO         = 5  # Log of ratio of adjusted low to previous day's adjusted close\n",
    "VOLUME_LOG            = 6  # Log of volume\n",
    "DID_INCREASE_AT_OPEN  = 7  # 1 if adjusted open price is greater than previous day's adjusted close price; 0 otherwise\n",
    "DID_INCREASE_AT_CLOSE = 8  # 1 if close price is greater than open price; 0 otherwise\n",
    "\n",
    "stats = []\n",
    "dates = []\n",
    "\n",
    "with open(STOCKS_CSV) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    index = 0\n",
    "    \n",
    "    for row in reader:\n",
    "        date_string = row[0]\n",
    "        if date_string == 'Date':\n",
    "            continue\n",
    "        \n",
    "        date = datetime.datetime.strptime(date_string, \"%Y-%m-%d\").date()\n",
    "        open_val = float(row[OPEN])\n",
    "        high_val = float(row[HIGH])\n",
    "        low_val = float(row[LOW])\n",
    "        close_val = float(row[CLOSE])\n",
    "        volume_val = float(row[VOLUME])\n",
    "        adj_close = float(row[ADJ_CLOSE])\n",
    "\n",
    "        if close_val != adj_close:\n",
    "            print(date_string, ' Adjustment =', adj_close / close_val)\n",
    "        \n",
    "        values = np.zeros((DID_INCREASE_AT_CLOSE + 1))\n",
    "        values[VOLUME_LOG] = np.log(volume_val)\n",
    "        values[DID_INCREASE_AT_CLOSE] = 1.0 if close_val > open_val else 0.0\n",
    "        \n",
    "        # Previous index is later date\n",
    "        if index > 0:\n",
    "            delta_date = (next_date - date).total_seconds() / 3600.0 / 24.0\n",
    "            values[DAYS_UNTIL_NEXT_TRADE] = delta_date\n",
    "            stats[index - 1][DAYS_SINCE_LAST_TRADE] = delta_date\n",
    "            stats[index - 1][OPEN_LOG_RATIO] = np.log(next_adj_open / adj_close)\n",
    "            stats[index - 1][CLOSE_LOG_RATIO] = np.log(next_adj_close_val / adj_close)\n",
    "            stats[index - 1][HIGH_LOG_RATIO] = np.log(next_adj_high_val / adj_close)\n",
    "            stats[index - 1][LOW_LOG_RATIO] = np.log(next_adj_low_val / adj_close)\n",
    "            stats[index - 1][DID_INCREASE_AT_OPEN] = 1.0 if next_adj_open > adj_close else 0.0\n",
    "\n",
    "        stats.append(values)\n",
    "        dates.append(date_string)\n",
    "        next_date = date\n",
    "        next_adj_open = open_val * adj_close / close_val\n",
    "        next_adj_high_val = high_val * adj_close / close_val\n",
    "        next_adj_low_val = low_val * adj_close / close_val\n",
    "        next_adj_close_val = adj_close\n",
    "        index += 1\n",
    "\n",
    "n = int(index * 0.2)\n",
    "test_stats = stats[1:n][::-1]\n",
    "train_stats = stats[n:-1][::-1]\n",
    "test_dates = dates[1:n][::-1]\n",
    "train_dates = dates[n:-1][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save stock index stats in a TFRecord file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCKS_TEST_OUT = './data/stock_test.tfrecords'\n",
    "STOCKS_TRAIN_OUT = './data/stock_train.tfrecords'\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _float_vector_feature(values):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _string_feature(value):\n",
    "    return _bytes_feature(value.encode('utf-8'))\n",
    "\n",
    "def write_stats(filename, dates_list, stats_list):\n",
    "    with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "        for (i, stat) in enumerate(stats_list):\n",
    "            date = dates_list[i]\n",
    "            feature_vector = stat[DAYS_SINCE_LAST_TRADE : VOLUME_LOG + 1]\n",
    "            open_inc = int(stat[DID_INCREASE_AT_OPEN])\n",
    "            close_inc = int(stat[DID_INCREASE_AT_CLOSE])\n",
    "\n",
    "            example = tf.train.Example(\n",
    "                features=tf.train.Features(\n",
    "                    feature={\n",
    "                        'statistics': _float_vector_feature(feature_vector),\n",
    "                        'date': _string_feature(date),\n",
    "                        'open_inc': _int64_feature(open_inc),\n",
    "                        'close_inc': _int64_feature(close_inc)\n",
    "                        }))\n",
    "            writer.write(example.SerializeToString())\n",
    "        \n",
    "write_stats(STOCKS_TEST_OUT, test_dates, test_stats)\n",
    "write_stats(STOCKS_TRAIN_OUT, train_dates, train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save normalization information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "STOCKS_NORM = './data/stock_norm.py'\n",
    "\n",
    "normalization = []\n",
    "normalization.append(np.mean(train_stats, axis=0))\n",
    "normalization.append(np.std(train_stats, axis=0))\n",
    "normalization.append(np.max(train_stats, axis=0))\n",
    "normalization.append(np.min(train_stats, axis=0))\n",
    "\n",
    "with open(STOCKS_NORM, 'wb') as file:\n",
    "    pickle.dump(normalization, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input text file for the BERT feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPECTED_COUNT = 25\n",
    "NEWS_TEST_TXT = './data/news_test.txt'\n",
    "NEWS_TRAIN_TXT = './data/news_train.txt'\n",
    "\n",
    "def write_news(filename, expected_count, dates_list):\n",
    "    with open(filename, 'w') as file:\n",
    "        for date_id in dates_list:\n",
    "            news_list = news_by_date[date_id]\n",
    "            news_feature_list = []\n",
    "\n",
    "            if len(news_list) != expected_count:\n",
    "                print(date_id, 'news count:', len(news_list), '- padding to', expected_count)\n",
    "\n",
    "            index = 0\n",
    "            for news_item in news_list:\n",
    "                file.write(news_item)\n",
    "                file.write('\\n')\n",
    "                index += 1\n",
    "\n",
    "            for i in range(index, expected_count):\n",
    "                file.write(' \\n')\n",
    "    \n",
    "write_news(NEWS_TEST_TXT, EXPECTED_COUNT, test_dates)\n",
    "write_news(NEWS_TRAIN_TXT, EXPECTED_COUNT, train_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_features import extract\n",
    "MODEL_DIR = './uncased_L-12_H-768_A-12'\n",
    "NEWS_TEST_OUT = './data/news_test.tfrecords'\n",
    "NEWS_TRAIN_OUT = './data/news_train.tfrecords'\n",
    "\n",
    "extract(input_file=NEWS_TEST_TXT, output_file=NEWS_TEST_OUT, bert_model_dir=MODEL_DIR, group_count=EXPECTED_COUNT)\n",
    "extract(input_file=NEWS_TRAIN_TXT, output_file=NEWS_TRAIN_OUT, bert_model_dir=MODEL_DIR, group_count=EXPECTED_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "Run the `NewsfeedTraining.ipynb` notebook next..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
